# 强化学习

## 概念

### state $s$

### action $a$

### policy $\pi$

策略函数：一种条件概率分布，基于当前状态采样不同的动作。 $A \sim \pi\left(a|s\right)$

### reward

基于当前action和state获得的奖励

### state translation

一种条件概率分布，基于当前action和state采样新的状态，$S\prime  \sim p(\cdot|s,a)$

### Agent-Environment Interaction

![image-20220606172841884](Image\image-20220606172841884.png)

### Return(aka cumulative future reward)

Return：$U_t = R_t + R_{t+1} + R_{t+2} + R_{t+3} + \cdots$

Discounted return: $U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots$

$R_i$也是随机变量，因为其依赖于$S_i$和$A_i$，$S_i \sim p(\cdot|s_{i-1}, a_{i-1}), A_i \sim \pi(\cdot|s_i)$因此$U_i$也是随机变量

小写字母表示观测值，如$u_t$、$r_t$等

### Value Functions 

#### Action-value function $Q_\pi\left(s,a\right)$

$Q_\pi\left(s,a\right) = \mathbb{E}\left[U_t | S_t = s_t, A_t = a_t\right]$,$Q_\pi\left(s,a\right)$依赖于$S_t, a_t, \pi, p$

#### Sate-value function $V_\pi\left(s\right)$

$V_\pi\left(s\right) = \mathbb{E_A}\left[Q_\pi\left(s_t,A\right)\right]$